{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## good for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the synthetic observed maps\n",
    " ---\n",
    "We load the foreground and cosmological signal simulations and combine them in a synthetic observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the main simulation from\n",
    "\n",
    "\n",
    "---\n",
    "It includes:\n",
    "1. Cosmological signal\n",
    "2. Galactic synchrotron\n",
    "3. Galactic free-free\n",
    "4. Extragalactic point sources and background\n",
    "5. Polarization leakage\n",
    "\n",
    "The cosmological signal and polarization leakage have been simulated with CRIME, the galactic foregrounds are generated from the Planck Sky Model, the point sources using the Battye et all 2013 model. Details are in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import healpy as hp\n",
    "\n",
    "import gmca4im_lib2 as g4i\n",
    "\n",
    "nu0 = 1420.4 #MHz, the 21cm emitted frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('../sim_PL05_from191030.hd5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['cosmological_signal', 'frequencies', 'gal_ff', 'gal_synch', 'point_sources', 'pol_leakage']>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = ['cosmological_signal', 'gal_ff', 'gal_synch', 'point_sources', 'pol_leakage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the frequencies/channels we work with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_ch = np.array(file['frequencies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with 400 channels, from 900.5 to 1299.5 MHz\n",
      "i.e. channels are 1.0 MHz thick\n",
      "corresponding to the redshift range z: [0.09 - 0.58] \n"
     ]
    }
   ],
   "source": [
    "print(f'working with {len(nu_ch)} channels, from {min(nu_ch)} to {max(nu_ch)} MHz')\n",
    "print(f'i.e. channels are {nu_ch[1]-nu_ch[0]} MHz thick')\n",
    "print(f'corresponding to the redshift range z: [{min(nu0/nu_ch -1.0):.2f} - {max(nu0/nu_ch -1.0):.2f}] ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the analysis, we merge maps in less but thicker channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging maps, going from 400 to 40 channels\n",
      "  i.e. from dnu=1.0 MHz to 10 MHz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dnu_out = 10 # MHz\n",
    "nu_ch_new = g4i.nu_ch_f(nu_ch,dnu_out)\n",
    "\n",
    "print(f'Merging maps, going from {len(nu_ch)} to {len(nu_ch_new)} channels')\n",
    "print(f'  i.e. from dnu={nu_ch[1]-nu_ch[0]} MHz to {dnu_out} MHz\\n')\n",
    "\n",
    "sim_dic = {}\n",
    "\n",
    "for c in components:\n",
    "    tmp = g4i.merging_maps(nu_ch,nu_ch_new,file[c],dnu_out)\n",
    "    sim_dic.update({c: tmp}); del tmp\n",
    "    \n",
    "nu_ch = nu_ch_new; del nu_ch_new\n",
    "del file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making noise realisation\n",
    "We choose survey parameters MeerKLASS-like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making noise realizations using given survey specifics:\n",
      "dish_diam  =  13.5  m\n",
      "T_inst  =  20.0  K\n",
      "f_sky  =  0.1  \n",
      "t_obs  =  4000.0  hours\n",
      "Ndishes  =  64.0  \n"
     ]
    }
   ],
   "source": [
    "survey_specs = 'MK'\n",
    "\n",
    "## initialise a dictionary with the instrument specifications\n",
    "## for noise and beam calculation\n",
    "specs_dict = g4i.survey_info(survey_specs)\n",
    "\n",
    "print('Making noise realizations using given survey specs:')\n",
    "for x,k in zip(specs_dict,[' m',' K',' ',' hours',' ']):\n",
    "    print(x, ' = ', specs_dict[x],k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnu = nu_ch[1]-nu_ch[0]\n",
    "nside_out = int(np.sqrt(np.shape(sim_dic['cosmological_signal'])[1] / 12))\n",
    "\n",
    "sigma_noise = g4i.sigma_N(nu_ch,dnu,**specs_dict)\n",
    "\n",
    "noise = [g4i.noise_map(sigma,nside=nside_out) for sigma in sigma_noise]\n",
    "del sigma_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telescope beam\n",
    "We convolve the maps with a Gaussian beam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing beam size using given survey specifics:\n",
      "\n",
      "0.983 < theta_beam [deg] < 1.407 \n",
      "\n",
      " convolving the cosmological_signal maps . . \n",
      " convolving the gal_ff maps . . \n",
      " convolving the gal_synch maps . . \n",
      " convolving the point_sources maps . . \n",
      " convolving the pol_leakage maps . . \n"
     ]
    }
   ],
   "source": [
    "print('Computing beam size using given survey specifics:')\n",
    "\n",
    "max_theta  = g4i.theta_FWHM(np.min(nu_ch),specs_dict['dish_diam']) \n",
    "min_theta  = g4i.theta_FWHM(np.max(nu_ch),specs_dict['dish_diam'])\n",
    "\n",
    "print('\\n%.3f < theta_beam [deg] < %.3f \\n'%(min_theta*180/np.pi,max_theta*180/np.pi))\n",
    "\n",
    "LMAX = 3*nside_out\n",
    "\n",
    "theta_ar  = g4i.theta_FWHM(nu_ch,specs_dict['dish_diam'])\n",
    "Beam_l_nu = [g4i.getBeam(theta_ar[i],LMAX) for i in range(len(nu_ch))]\n",
    "\n",
    "for c in components:\n",
    "    print(f'  convolving the {c} maps . . ')\n",
    "    # convolve with a frequency-dependent beam\n",
    "    tmp_out = [g4i.convolve(sim_dic[c][i],Beam_l_nu[i],LMAX) for i in range(len(nu_ch))]\n",
    "    sim_dic.update({c: tmp_out}); del tmp_out\n",
    "\n",
    "del Beam_l_nu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing the Blind Source Separation, we make all maps share the same resolution,\n",
    "i.e. we downgrade maps to worst beam (lower frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing again to same resolution . .\n",
      "  also the noise maps gets smoothed\n"
     ]
    }
   ],
   "source": [
    "print('Smoothing again to same resolution . .')\n",
    "beam_to_worst = [g4i.getBeam(np.sqrt(max_theta**2-theta_ar[i]**2),LMAX) for i in range(len(nu_ch))]\n",
    "\n",
    "for c in components:\n",
    "    # smooth again to a same resolution\n",
    "    print(f'  de-convolving the {c} maps . . ')\n",
    "    tmp_out = [g4i.convolve(sim_dic[c][i],beam_to_worst[i],LMAX) for i in range(len(nu_ch))]\n",
    "    sim_dic.update({c: tmp_out}); del tmp_out\n",
    "\n",
    "print('  also the noise maps gets smoothed . .')\n",
    "# doing same thing on the noise maps\n",
    "noise_tmp = [g4i.convolve(noise[i],beam_to_worst[i],LMAX) for i in range(len(nu_ch))]\n",
    "del noise; noise = noise_tmp; del noise_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"frequencies\": shape (40,), type \"<f8\">"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_dic.update({'noise': noise})\n",
    "\n",
    "file_out = h5py.File('../sim_10MHz.hd5','w')\n",
    "for c in components:\n",
    "    file_out.create_dataset(c, data=sim_dic[c])\n",
    "    \n",
    "file_out.create_dataset('noise', data=noise)\n",
    "file_out.create_dataset('frequencies', data=nu_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOW SOME PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the wavelet transform of the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
